<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Ardhendu Behera</title>
    <link href="css/main.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Dosis" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Merriweather" rel="stylesheet">
</head>
<body>
<div class="header-content">
    <nav class="nav">
       <a href="index.html"><img alt="Ardhendu Behera" id="mainImage" src="img/ardhendu-behera.jpg"></a>
        <div class="rightNav">
            <a href="pastnews.html">Past News</a>
            <a href="projects.html">Past Research Projects</a>
            <a href="teaching.html">Teaching</a>
            <a href="outreach.html">Outreach/Media Coverage</a>
            <a href="talks.html">Keynote/Invited Talks</a>
            <a href="team.html">Team Members</a>
            <a href="contact.html">Contact</a>
        </div>
        <!--
        <div class="nav">   
             Dr Ardhendu Behera,  Reader (Associate Professor) in Computer Vision and Artificial Intelligence, 
            <a href="https://www.edgehill.ac.uk/computerscience/">Department of Computer Science</a>, <a href="https://www.edgehill.ac.uk/">Edge Hill University</a>
        </div>-->
    </nav>
</div>
<div class="body-content">
    <article class="articleLayout">
        <p>
             Dr Ardhendu Behera,  Reader (Associate Professor) in Computer Vision and Artificial Intelligence<br> 
            <a href="https://www.edgehill.ac.uk/computerscience/">Department of Computer Science</a>, <a href="https://www.edgehill.ac.uk/">Edge Hill University</a>
        </p>
        <h2>Past Research Projects</h2>        
         <h3>Autonomous Intelligent Feature for Understanding Drivers' Behaviour Pattern</h3>
        <p>
          <img src="img/RIF15.jpg" alt="RIF" width="390" height="210" align="left" hspace="10"> <b>Funded by: Research Investment Funding (RIF), Edge Hill University </b>. 
          The purpose of the project is to monitor drivers activity in real-time. The aim is to better summarise and develop behaviour models of individual drivers.<br><br>
			A. Behera and A. H. Keidel, <b>Latent Body-Pose guided DenseNet for Recognizing Driver’s Fine-grained Secondary Activities</b>. 
			15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS) 2018, DOI: 
			<a href="https://doi.org/10.1109/AVSS.2018.8639158" target="_parent">10.1109/AVSS.2018.8639158</a>, 
			<a href="http://repository.edgehill.ac.uk/10783/1/Latent-Body-Behera%20%28002%29.pdf" target="_parent">PDF</a>. <br><br>
			A. Behera, A. H. Keidel and B. Debnath, <b>Context-driven Multi-stream LSTM (M-LSTM) for Recognizing Fine-Grained Activity of Drivers</b>.
			40th German Conference on Pattern Recognition (GCPR) 2018, DOI: 
			<a href="https://doi.org/10.1007/978-3-030-12939-2_21" target="_parent">10.1007/978-3-030-12939-2_21</a>, 
			<a href="http://repository.edgehill.ac.uk/10782/1/0050.pdf" target="_parent">PDF</a>. 
        </p>
        <h3>Human-Robot Social Interactions </h3>
	    <p>
            <img src="img/human-robot.jpg" alt="HRI" width="390" height="210" align="left" hspace="10"/>
            A. Behera, A. G. Gidney, Z. Wharton, D. Robinson and K. Quinn, <b>A CNN Model for Head Pose Recognition using Wholes and Regions</b>. 
		    IEEE International Conference on Automatic Face and Gesture Recognition (FG 2019). <a href="https://research.edgehill.ac.uk/ws/files/20225095/Camera_Ready_FG2019.pdf">
		    <b>Preprint</b></a>, DOI: <a href="https://doi.org/10.1109/FG.2019.8756536 target="_parent">10.1109/FG.2019.8756536</a><br> <br>
			Z. Wharton, E.Thomas, B. Debnath and A. Behera, <b>A Vision-based Transfer Learning Approach for Recognizing Behavioral Symptoms in People with Dementia</b>.
			 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS) 2018.  
			    <a href="https://research.edgehill.ac.uk/ws/files/20125875/Vision-based-Behera.pdf"><b>Preprint</b></a>, DOI: 
			<a href="https://doi.org/10.1109/AVSS.2018.8639371" target="_parent">10.1109/AVSS.2018.8639371</a>, 
			<a href="http://repository.edgehill.ac.uk/10786/1/Vision-based-Behera.pdf" target="_parent">PDF</a>, Dataset. <br><br>
			B. Debnath, M. O’Brien, M. Yamaguchi and A. Behera, <b>Adapting MobileNets for mobile based upper body pose estimation</b>. 
			15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS) 2018. 
			    <a href="https://research.edgehill.ac.uk/ws/files/20126254/adapting-mobilenets-debnath.pdf"><b>Preprint</b></a>,DOI:
			<a href="https://doi.org/10.1109/AVSS.2018.8639378" target="_parent">10.1109/AVSS.2018.8639378</a>, 
			<a href="http://repository.edgehill.ac.uk/10789/1/adapting-mobilenets-debnath.pdf" target="_parent">PDF</a>. 
		</p>
	<h3>Activity Recognition and Monitoring </h3>
	<p>
            <img src="img/Activity-Recognition.jpg" alt="Activity Recognition" width="390" height="210" align="left" hspace="10"/> G.Bleser, D. Damen, A. Behera, 
			    G. Hendeby, K. Mura, M. Miezal et al., <b>Cognitive learning, monitoring and assistance of industrial workflows using egocentric sensor networks</b>.
			PLoS ONE 10(6): e0127769, 2015, DOI: <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0127769" target="_parent">10.1371/journal.pone.0127769</a>. <br><br>
			A. Behera, A.G. Cohn and D.C. Hogg, <b>Real-time Activity Recognition by Discerning Qualitative Relationships Between Randomly Chosen Visual Features</b>.  
			The 25th British Machine Vision Conference (BMVC 2014), <a href="http://eprints.whiterose.ac.uk/83878/1/bmvc_final.pdf" target="_parent">PDF</a>. <br><br> 
			A. Behera, M. Chapman, D.C. Hogg and A.G. Cohn, <b>Egocentric Activity Recognition using Histograms of Oriented Pairwise Relations</b>.
			 The 9th International Conference on Computer Vision Theory and Applications (VISAPP 2014), DOI: 
			<a href="https://ieeexplore.ieee.org/abstract/document/7294910" target="_parent"> IEEE Xplore</a>, 
			<a href="http://eprints.whiterose.ac.uk/81155/1/visapp-14.pdf" target="_parent">PDF</a>. <br><br>
			    A. Behera, D.C. Hogg and A.G. Cohn, <b>Egocentric activity monitoring and recovery</b>. 
			Asian Conference on Computer Vision (ACCV 2012), DOI:
			<a href="https://doi.org/10.1007/978-3-642-37431-9_40" target="_parent">10.1007/978-3-642-37431-9_40</a>, 
			<a href="http://eprints.whiterose.ac.uk/81161/1/PaperAccv2012-final.pdf" target="_parent">PDF</a>. <br><br>
			A. Behera, A.G. Cohn and D.C. Hogg, <b>Workflow activity monitoring using dynamics of pair-wise qualitative spatial relations</b>.
			International Conference on Multimedia Modeling (MMM 2012), DOI:
			<a href="https://doi.org/10.1007/978-3-642-27355-1_20" target="_parent">10.1007/978-3-642-27355-1_20</a>, 
			<a href="http://eprints.whiterose.ac.uk/75313/12/hoggd1.pdf" target="_parent">PDF</a>. <br><br>
			S.F. Worgan, A. Behera, A.G. Cohn and D.C. Hogg, <b>Exploiting petri-net structure for activity classification and user instruction within an industrial setting</b>. 
			ACM International Conference on Multimodal Interfaces (MLMI 2011), DOI:
			<a href="https://doi.org/10.1145/2070481.2070502" target="_parent">10.1145/2070481.2070502</a>, 
			<a href="https://institutes.engineering.leeds.ac.uk/computing/research/vision/cognito/icmi244-beheraPS.pdf" target="_parent">PDF</a>. 
	</p>
	 <h3>Adaptive User Interface: Eye-based Human-Computer Interaction (HCI) </h3>
	<p>
            <img src="img/RITA16.jpg" alt="RITA" width="390" height="210" align="left" hspace="10"/>
			    <b> Funded by: Research Institute thematic Award (RITA), Edge Hill University</b>. Partners: <a href="https://www.edgehill.ac.uk/psychology/" target="_parent">Psychology, Edge Hill University</a>, 
			<a href="https://www.thewaltoncentre.nhs.uk/" target="_parent"> The Walton Centre NHS Foundation Trust</a>, 
			<a href="http://www.southportandormskirk.nhs.uk/" target="_parent">Southport & Ormskirk Hospital NHS Trust</a><br><br>
            The aim of this multi-disciplinary research project is to develop an adaptive user interface that can be used by disable people 
			suffering from Spinal Cord Injury (SCI) or Motor Neurone Disease (MND). The adaptive nature of the interface is based 
			    on the severity of the disability and can have a distinct impact on such people to express themselves in conversation 
			    with their caregiver and families.
	</p>
	<h3>Intelligent Video Surveillance </h3>
	<p>
            <img src="img/surveillance-cctv.jpg" alt="CCTV" width="390" height="210" align="left" hspace="10"/>
            Ferryman, D.C. Hogg, J. Sochman, A. Behera, J.A. Rodriguez-Serrano, S. Worgan et al., <b>Robust abandoned object detection integrating wide area visual 
			    surveillance and social context</b>. Pattern Recognition Letters (PRL) 34(7), 2013, DOI: <a href="https://doi.org/10.1016/j.patrec.2013.01.018" target="_parent">10.1016/j.patrec.2013.01.018</a>, 
			<a href="http://eprints.whiterose.ac.uk/75462/17/hoggdc5.pdf" target="_parent">PDF</a>. <br><br>
			C.J. Howard, T. Troscianko, I.D. Gilchrist, A. Behera, D.C. Hogg, <b>Suspiciousness perception in dynamic scenes: a comparison of CCTV operators and novices</b>.
			Frontiers in human neuroscience, vol 7, 2013, DOI: <a href="https://doi.org/10.3389/fnhum.2013.00441" target="_parent">10.3389/fnhum.2013.00441</a>. <br><br> 
			C.J. Howard, T. Troscianko, I.D. Gilchrist, A. Behera, D.C. Hogg, <b>Task relevance predicts gaze in videos of real moving scenes</b>.
			Experimental brain research 214(1), 2011, DOI: <a href="https://doi.org/10.1007/s00221-011-2812-y" target="_parent">10.1007/s00221-011-2812-y</a>. <br><br>
			A. Behera, D.C. Hogg, C.J. Howard, I.D. Gilchrist and T. Troscianko, <b>Visual attention-based approach for prediction of abnormalities in CCTV video surveillance</b>.
			Applied Vision Association Meetings 2011, 41(3). <br><br>
			
			C.J. Howard, T. Troscianko, I.D. Gilchrist, A. Behera, D.C. Hogg, <b>Searching for threat: factors determining performance during CCTV monitoring</b>.
			Human factors, security and safety 2009, <a href="http://eis.bris.ac.uk/~psidg/download/HTGBH2009.pdf" target="_parent">PDF</a>. 
	</p>
	<h3>Learning to Recognise Dynamic Visual Content from Broadcast Footage</h3>
	<p>
	 <b>Funded by EPSRC, Partners:</b> <a href="http://www.robots.ox.ac.uk/~vgg/" target="_parent"> University of Oxford </a> and <a href="http://www.surrey.ac.uk/cvssp/" target="_parent">University of Surrey</a>. 
         The purpose of the project is to learn actions and activity from large volumes
of video data and associated subtitles and other metadata. In order to better summarise and
navigate these large volumes of video data, an important aspect of the project is to develop
behaviour models of individual characters. <a href="http://personal.ee.surrey.ac.uk/Personal/R.Bowden/DynaVis/index.html" target="_parent">More</a>.
	</p>
           
         <h3>Cognitive Workflow Capturing and Rendering with On-Body Sensor Networks (COGNITO)</h3>
	<p>
	<img src="img/packaging_vid.gif" width="200" height="200" align="left" hspace="10"><img src="img/pairwise_relation_state.gif" width="200" height="200" align="left" hspace="10"/>
			    <img src="img/pairwise_relation_half.gif" width="200" height="200" align="left" hspace="10"/> 
			    <b> Funded by: EU FP7, Partners:</b> <a href="http://av.dfki.de/" target="_parent">DFKI</a>, <a href="http://www.cs.bris.ac.uk/Research/Vision/" target="_parent">University of Bristol</a>, 
			<a href="http://www.utc.fr/" target="_parent">CNRS</a>, <a href="http://www.trivisio.com/" target="_parent">Trivisio</a>, <a href="http://www.ccg.pt/" target="_parent">CCG</a> and <a href="http://www.smartfactory-kl.de/" target="_parent">SmartFactory</a>
			    <br><br>
            The COGNITO project aims to develop principles, representations, formalisms and frameworks that will allow us to
build systems capable of observing, monitoring and learning from human users. The focus of the project on the development of technology which will enable systems to
automatically capture, learn and render user cognitive behavior and activity. <a href="http://www.ict-cognito.org/" target="_parent">More</a>
	</p>
        
	<h3>Cognitive Systems Foresight: Human Attention and Machine Learning (HAML)</h3>
	 <p>
            <img src="img/haml.gif" alt="Image 07" width="200" height="200" align="left" hspace="10"> <b>Funded by EPSRC and Wellcome Trust, Partner:</b> <a href="http://www.bris.ac.uk/expsych/" target="_parent">University of Bristol </a>. 
	    The main goal of this research is to develop a cognitive model that can explain human behaviour in a task involving naturalistic stimuli and high cognitive load. 
	    This would help in taking necessary measures to prevent any potential crimes. The novelties in our approach are mainly: (1) study of human behaviour on a realistic task such as 
	    threat assessment within a CCTV control room, (2) develop a model that incorporates the advanced computer vision and machine learning 
	    techniques for recognition of objects and their activities and (3) apply this model to optimise the CCTV control room and explore partial automation.
	</p>
			    
        <h3>Surveillance of Unattended Baggage and the Identification and Tracking of the Owner (SUBITO)</h3>
	<p>
	<img src="img/subito3.jpg" width="200" height="200" align="left" hspace="10"/><img src="img/subito2.jpg" width="200" height="200" align="left" hspace="10"/>
	<img src="img/subito1.jpg" width="200" height="200" align="left" hspace="10"/>
	<b>Funded by: EU FP7, Partners:</b> Selex Galileo Ltd, Selex Elsag Spa, ONERA, L-1, CEA, University of Reading, VTT, Austrian Institute of Technology, Fiera di Genova, University of Oxford.<br><br>
         The SUBITO project was aimed to built a system that would provide automated real-time detection of abandoned baggage, identification of the individual who left the baggage and the fast determination of the current location of that individual or his/her followed path. 
         It is guided by end-user requirements to ensure that security personnel receive the technologies they need in order to deliver improved threat security.
            <a href="http://www.subito-project.eu/" target="_parent">More</a>
	</p>		    
          			    
        <h3>Image Analysis for analysing growth rate of individual face of crystalline particles in microscopic images</h3>	
	<p>
         <img src="img/crystal.gif" width="200" height="200" align="left" hspace="10" align="left" hspace="10"/><img src="img/hexagon_crystal.jpg" width="200" height="200" align="left" hspace="10" align="left" hspace="10"/>
	 <img src="img/contour_crystal.jpg" align="left" hspace="10" align="left" hspace="10"/><img src="images/crystal.jpg" align="left" hspace="10" align="left" hspace="10"/><br><br>
	<b>Partners:</b> Malvern Instruments  Ltd, SPME at the University of Leeds. The main goal of this research is to develop a model for automatic detection of 
	individual crystalline particle in microscopic images and is able to provide the growth of each face of the detected crystalline particle over time.
	</p>
            
         <h3>Interactive Multimodal Information Management (IM2)</h3>
	<p>
           <img src="img/im2.gif" width="200" height="200" align="left" hspace="10"/><img src="img/image336.jpg" width="200" height="200" align="left" hspace="10"/><b>Funded by: Swiss National Science Foundation (SNSF), 
	  Partners:</b> EPFL, University of Geneva, University of Bern, ETHZ. <br><br>
          IM2 is concerned with the development of natural multimodal interfaces for human-computer interaction. Multimodal refers to 
	  the different technologies that coordinate natural input modes (such as speech, pen, touch, hand gestures, head and body movements, 
	and eventually physiological sensors) with multimedia system output (such as speech, sounds, and images). ltimately, these multimodal 
	interfaces should flexibly accommodate a wide range of users, tasks, and environments for which any single mode may not suffice. 
	The ideal interface should primarily be able to deal with more comprehensive and realistic forms of data, including mixed data 
	types (i.e., data from different input modalities such as image and audio). <a href="http://www.im2.ch/" target="_parent">More</a>
	</p>
</div>
<div class="footer-content">
<p style="text-align:center">2021 © Ardhendu Behera</p>
</div>

</body>
</html>
